{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Number classification algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read database\n",
    "\n",
    "Functions used to read the database (train and test) from MNIST and also from the csv file generated with the images captured from the pico camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digit:\n",
    "    def __init__(self, data, target):\n",
    "        self.target = target\n",
    "        self.width  = int(np.sqrt(len(data)))\n",
    "        self.image  = data.reshape(self.width, self.width)\n",
    "        self.features = {'var' : 0,\n",
    "                         'std' : 0,\n",
    "                         'mean_grad_M' : 0,\n",
    "                         'std_grad_M'  : 0,\n",
    "                         'mean_grad_D' : 0,\n",
    "                         'std_grad_D'  : 0,\n",
    "                         'mean_PC_X'   : 0,\n",
    "                         'std_PC_X'    : 0,\n",
    "                         'active_PC_X' : 0,\n",
    "                         'mean_PC_Y'   : 0,\n",
    "                         'std_PC_Y'    : 0,\n",
    "                         'active_PC_Y' : 0}\n",
    "        self.computeFeatures()\n",
    "    \n",
    "    def computeFeatures(self):\n",
    "        # Feature computation\n",
    "        mag, ang = sobel(self.image)\n",
    "        pcx, pcy = pixel_count(self.image)\n",
    "        \n",
    "        self.features['var'] = np.var(self.image)\n",
    "        self.features['std'] = np.std(self.image)\n",
    "        self.features['mean_grad_M'] = np.mean(mag)\n",
    "        self.features['std_grad_M'] =  np.std(mag)\n",
    "        self.features['mean_grad_D'] = np.mean(ang)\n",
    "        self.features['std_grad_D'] =  np.std(ang)\n",
    "        self.features['mean_PC_X'] =   np.mean(pcx)\n",
    "        self.features['std_PC_X'] =    np.std(pcx)\n",
    "        self.features['active_PC_X'] = np.count_nonzero(pcx)\n",
    "        self.features['mean_PC_Y'] =   np.mean(pcy)\n",
    "        self.features['std_PC_Y'] =    np.std(pcy)\n",
    "        self.features['active_PC_Y'] = np.count_nonzero(pcy) \n",
    "  \n",
    "    def __print__(self):\n",
    "        print(\"Digit target: \"+str(self.target))\n",
    "        print(\"Digit features:\")\n",
    "        print(self.features)\n",
    "        print(\"Digit image:\")\n",
    "        plt.gray()\n",
    "        plt.matshow(self.image) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel(image):\n",
    "    w = len(image)\n",
    "    kernel_x = np.array([ [ 1, 0,-1],\n",
    "                          [ 2, 0,-2],\n",
    "                          [ 1, 0,-1] ])\n",
    "\n",
    "    kernel_y = np.array([ [ 1, 2, 1],\n",
    "                          [ 0, 0, 0],\n",
    "                          [-1,-2,-1] ])\n",
    "    \n",
    "    grad_x = np.zeros([w - 2, w - 2])\n",
    "    grad_y = np.zeros([w - 2, w - 2])\n",
    "    \n",
    "    for i in range(w - 2):\n",
    "        for j in range(w - 2):\n",
    "            grad_x[i, j] = sum(sum(image[i : i + 3, j : j + 3] * kernel_x))\n",
    "            grad_y[i, j] = sum(sum(image[i : i + 3, j : j + 3] * kernel_y))\n",
    "            if grad_x[i, j] == 0:\n",
    "                grad_x[i, j] = 0.000001 \n",
    "    \n",
    "    mag = np.sqrt(grad_y ** 2 + grad_x ** 2)\n",
    "    ang = np.arctan(grad_y / (grad_x + np.finfo(float).eps))\n",
    "  \n",
    "    # Gradient computation\n",
    "  \n",
    "    return [mag,ang]\n",
    "\n",
    "def pixel_count(image):\n",
    "    pc_x = np.zeros(len(image))\n",
    "    pc_y = np.zeros(len(image))\n",
    "  \n",
    "    # Pixel count computation\n",
    "    for i in range(len(image)):\n",
    "        pc_x[i] = np.count_nonzero(image[i, :])\n",
    "        pc_y[i] = np.count_nonzero(image[:, i])\n",
    "\n",
    "    return [pc_x, pc_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, array, length):  \n",
    "        self.array = array\n",
    "        self.length = length\n",
    "        self.digits = []\n",
    "        self.digits = self.createDigits()\n",
    "        self.raw_features = [[float(f) for f in dig.features.values()] for dig in self.digits]\n",
    "        self.raw_targets  = [[self.digits[i].target] for i in range(self.length)]\n",
    "  \n",
    "    def createDigits(self):\n",
    "        digits = []\n",
    "        for row in self.array:\n",
    "            digits.append(Digit(np.array(row[:-1]), row[-1]))\n",
    "                \n",
    "        return digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n : percent of the number of lines in the data base use as array set\n",
    "def load_data_set(array, n):\n",
    "    dataset = Dataset(array[:int(n * len(array))], int(n * len(array)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 ms, sys: 274 ms, total: 279 ms\n",
      "Wall time: 277 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Load the database (.npy) files \n",
    "img_array_train = np.load('./../../train.npy')\n",
    "img_array_test = np.load('./../../test.npy')\n",
    "img_array_validation = np.load('./../../validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset...\n",
      "Number of the lines in the train dataset: 120000\n",
      "Number of the lines in the train set: 120000\n",
      "\n",
      "Finished creating train dataset\n",
      "\n",
      "CPU times: user 23min 16s, sys: 3.49 s, total: 23min 19s\n",
      "Wall time: 23min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "print(\"Creating train dataset...\")\n",
    "print(\"Number of the lines in the train dataset: \" + str(len(img_array_train))) \n",
    "training_set = load_data_set(img_array_train, 1)\n",
    "print(\"Number of the lines in the train set: \" + str(training_set.length))\n",
    "print (\"\\nFinished creating train dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing dataset\n",
      "Number of the lines in the test dataset: 6150\n",
      "Number of the lines in the test dataset: 6150\n",
      "\n",
      "Finished creating test dataset\n",
      "\n",
      "CPU times: user 1min 9s, sys: 152 ms, total: 1min 9s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "print(\"Creating testing dataset\")\n",
    "print(\"Number of the lines in the test dataset: \" + str(len(img_array_test)))\n",
    "testing_set = load_data_set(img_array_test, 1)\n",
    "print (\"Number of the lines in the test dataset: \" + str(testing_set.length))\n",
    "print (\"\\nFinished creating test dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset...\n",
      "Number of the lines in the validation dataset: 14350\n",
      "Number of the lines in the validation dataset: 14350\n",
      "\n",
      "Finished creating validation dataset\n",
      "\n",
      "CPU times: user 2min 42s, sys: 332 ms, total: 2min 43s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "    \n",
    "print(\"Creating validation dataset...\")\n",
    "print(\"Number of the lines in the validation dataset: \" + str(len(img_array_validation)))\n",
    "validation_set = load_data_set(img_array_validation, 1)\n",
    "print (\"Number of the lines in the validation dataset: \" + str(validation_set.length))\n",
    "print (\"\\nFinished creating validation dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(0,5):\n",
    "    validation_set.digits[i].__print__()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt_obj_nparray(dataset):\n",
    "    X = np.zeros((dataset.length, 12))\n",
    "    Y = np.zeros((dataset.length,))\n",
    "    for i, digit in enumerate(dataset.digits):\n",
    "        Y[i] = digit.target\n",
    "        for j, feature in enumerate(digit.features):\n",
    "            X[i, j] = digit.features[feature]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating X_dataset and Y_dataset\n",
      "Finished X_dataset and Y_dataset\n",
      "\n",
      "CPU times: user 525 ms, sys: 12 ms, total: 537 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#The X_dataset and Y_dataset are the lists used in the classifier\n",
    "print(\"Creating X_dataset and Y_dataset\")\n",
    "X_train, Y_train = cvt_obj_nparray(training_set)\n",
    "X_test, Y_test = cvt_obj_nparray(testing_set)\n",
    "X_validation, Y_validation = cvt_obj_nparray(validation_set)\n",
    "print(\"Finished X_dataset and Y_dataset\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(clf, data_x, data_y):\n",
    "    classes = string.ascii_uppercase\n",
    "    \n",
    "    Ypred = clf.predict(data_x)\n",
    "    \n",
    "    # Computation of confusion matrix on testing set\n",
    "    cm = confusion_matrix(data_y, Ypred)\n",
    "    \n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest classifier...\n",
      "Finished trainning Random Forest classifier with 100 estimators\n"
     ]
    }
   ],
   "source": [
    "def train_randomForestModel(i):  \n",
    "    #Train the classifier\n",
    "    print(\"Training Random Forest classifier...\")\n",
    "    clf = RandomForestClassifier(n_estimators=i)\n",
    "    print(\"Finished trainning Random Forest classifier with \" + str(i) + \" estimators\")\n",
    "    \n",
    "    # Training DecisionTree\n",
    "    return clf.fit(X_train, Y_train)\n",
    "\n",
    "randomForest_classificator = train_randomForestModel(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction accuracy on training set\n",
    "def visualize_randomForestModel():\n",
    "    predicted = randomForest_classificator.predict(X_train)\n",
    "    print(\"The accuracy for test database is: \" + str(round((accuracy_score(Y_train, predicted) * 100), 3)) + \"%\")\n",
    "\n",
    "    print(\"Using: \" + str(len(X_train)) + \" images\")\n",
    "    \n",
    "    plot_confusion_matrix(randomForest_classificator, X_train, Y_train)\n",
    "    plt.title('Train confusion matrix: accuracy of ' + str(round((accuracy_score(Y_train, predicted) * 100), 3)) + \"%, used \" + str(len(X_train)) + \" images\")\n",
    "    plt.savefig('./train')\n",
    "\n",
    "visualize_randomForestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction accuracy on validation set\n",
    "def validation_randomForestModel():\n",
    "    predicted = randomForest_classificator.predict(X_validation)\n",
    "    print(\"The accuracy for test database is: \" + str(round((accuracy_score(Y_validation, predicted) * 100), 3)) + \"%\")\n",
    "\n",
    "    print(\"Using: \" + str(len(X_validation)) + \" images\")\n",
    "    \n",
    "    plot_confusion_matrix(randomForest_classificator, X_validation, Y_validation)\n",
    "    plt.title('Validation confusion matrix: accuracy of ' + str(round((accuracy_score(Y_validation, predicted) * 100), 3)) + \"%, used \" + str(len(X_validation)) + \" images\")\n",
    "    plt.savefig('./validation')\n",
    "\n",
    "validation_randomForestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation methods**\n",
    "\n",
    "This part contains the functions used to analyse the accuracy of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiple Hyperparameter Study**\n",
    "This part consist in making a study of the hyperparameter of the classifier, using the gridsearch method from the scikitlearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter values that should be searched\n",
    "estimators_range = list(range(1, 120, 10))\n",
    "\n",
    "# Another parameter besides max_depth that we might vary is the criteria\n",
    "criterion_options = ['gini', 'entropy']\n",
    "#features_options = ['auto', 'sqrt','log2', None]\n",
    "\n",
    "# Specify \"parameter grid\"\n",
    "param_grid = dict(n_estimators=estimators_range, criterion=criterion_options)\n",
    "\n",
    "# Specify \"parameter distributions\" rather than a \"parameter grid\"\n",
    "param_dist = dict(n_estimators=estimators_range, criterion=criterion_options)\n",
    "\n",
    "# Since both parameters are discrete, so param_dist is the same as param_grid\n",
    "\n",
    "# Finally define the classifier, in this case the DecisionTree classifier\n",
    "randomForest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_best_scores = []\n",
    "grid_best_params = []\n",
    "grid_best_estimator = []\n",
    "\n",
    "rand_best_scores = []\n",
    "rand_best_params = []\n",
    "rand_best_estimator = []\n",
    "\n",
    "for i in list(range(20)):\n",
    "    print(str(i))\n",
    "    grid = GridSearchCV(randomForest, param_grid, cv=10, scoring='accuracy', n_jobs = -1)\n",
    "    grid.fit(X_train, Y_train)\n",
    "    grid_best_scores.append(grid.best_score_)\n",
    "    grid_best_params.append(grid.best_params_)\n",
    "    grid_best_estimator.append(grid.best_estimator_)\n",
    "\n",
    "    rand = RandomizedSearchCV(randomForest, param_dist, cv=10, scoring='accuracy', n_iter=10, n_jobs = -1)\n",
    "    rand.fit(X_train, Y_train)\n",
    "    rand_best_scores.append(rand.best_score_)\n",
    "    rand_best_params.append(rand.best_params_)\n",
    "    rand_best_estimator.append(rand.best_estimator_)\n",
    "    \n",
    "print(\"Finished searching the hyper parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block draw accuracies of the differents classifiers used in the grid and random search\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title(\"Final search results\")   \n",
    "plt.plot(range(len(grid_best_scores)), grid_best_scores, label = 'Exaustive grid')\n",
    "plt.plot(range(len(rand_best_scores)), rand_best_scores, label = 'Random grid')\n",
    "plt.xlabel('Number of classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(grid_best_scores)))\n",
    "plt.show()\n",
    "\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"The best accuracy is: \" + str(np.max(grid_best_scores) * 100) + \"%\")\n",
    "print(\"The parameters is: \" + str(grid_best_params[np.argmax(grid_best_scores)]))\n",
    "print('\\n')\n",
    "print(\"RANDOMGRID SEARCH RESULTS\")\n",
    "print(\"The best accuracy is: \" + str(np.max(rand_best_scores) * 100) + \"%\")\n",
    "print(\"The parameters is: \" + str(rand_best_params[np.argmax(rand_best_scores)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best DecisionTree classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the best classificator for a N sample of numbers\n",
    "def train_best_classifier():   \n",
    "    classificator = rand_best_estimator[np.argmax(rand_best_scores)]\n",
    "\n",
    "    return classificator.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_randomForest_classificator = train_best_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction accuracy on testing set\n",
    "def test_randomForestModel():\n",
    "    predicted = best_randomForest_classificator.predict(X_test)\n",
    "    print(\"The accuracy for test database is: \" + str(round((accuracy_score(Y_test, predicted) * 100), 3)) + \"%\")\n",
    "\n",
    "    print(\"Using: \" + str(len(X_test)) + \" images\")\n",
    "    \n",
    "    plot_confusion_matrix(randomForest_classificator, X_test, Y_test)\n",
    "    plt.title('Test confusion matrix: accuracy of ' + str(round((accuracy_score(Y_test, predicted) * 100), 3)) + \"%, used \" + str(len(X_test)) + \" images\")\n",
    "    plt.savefig('./test')\n",
    "\n",
    "test_randomForestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "filename = './../../aplication/randomForest/randomForest_classifier.sav'\n",
    "joblib.dump(randomForest_classificator, filename, compress = 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best classifier\n",
    "filename = './../../aplication/randomForest/best_randomForest_classifier.sav'\n",
    "joblib.dump(best_randomForest_classificator, filename, compress = 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features study for Random Forest classifier\n",
    "\n",
    "In a Random Forest model the feature used to make the classification is very important, so in this part it is made a study on the best feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part displays the table of importance of each feature\n",
    "\n",
    "bins = np.arange(len(training_set.digits[0].features))\n",
    "label_list = []\n",
    "for feature in training_set.digits[0].features:\n",
    "    label_list.append(feature)\n",
    "\n",
    "importance = best_randomForest_classificator.feature_importances_\n",
    "df = pd.DataFrame({'feature': label_list, 'Level of importance': importance})\n",
    "df.sort_values(by = ['Level of importance']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part plots a grafic for better visualization\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(bins, importance)\n",
    "ax.set_xticks(np.arange(len(label_list)))\n",
    "ax.set_yticks([])\n",
    "ax.set_xticklabels(label_list, rotation = 45, ha = 'right')\n",
    "# Display bar graph of feature importances with feature names as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
